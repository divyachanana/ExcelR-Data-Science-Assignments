# -*- coding: utf-8 -*-
"""simple linear regression salary hike

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1EBFz6xVr1-jEfnFlAgyiVzSXxc7IsRIC

Importing libraries
"""

import pandas as pd
import numpy as np
import scipy.stats as stats
import matplotlib.pyplot as plt
import seaborn as sns
import statsmodels.api as smf
import statsmodels.formula.api as sm
import warnings
warnings.filterwarnings('ignore')

"""Step 1:
Importing data
"""

df = pd.read_csv('Salary_Data.csv')
df

"""Step 2:
Performing EDA On Data

Checking Data Type
"""

df.info()

df.describe()

"""Checking for Null Values

"""

df.isnull().sum()

"""Checking for Duplicate Values"""

df[df.duplicated()].shape

df[df.duplicated()]

"""Step 3:
Plotting the data to check for outliers
"""

plt.subplots(figsize = (9,6))
plt.subplot(121)
plt.boxplot(df['Salary'])
plt.title('Salary Hike')
plt.subplot(122)
plt.boxplot(df['YearsExperience'])
plt.title('Years of Experience')
plt.show()

"""As you can see there are no Outliers in the data

Step 4:
Checking the Correlation between variables
"""

df.corr()

"""Visualization of Correlation beteen x and y

regplot = regression plot
"""

sns.regplot(x=df['YearsExperience'],y=df['Salary'])

"""As you can see above

There is good correlation between the two variable.

The score is more than 0.8 which is a good sign

Step 5:
Checking for Homoscedasticity or Hetroscedasticity
"""

plt.figure(figsize = (8,6), facecolor = 'lightgreen')
sns.scatterplot(x = df['YearsExperience'], y = df['Salary'])
plt.title('Homoscedasticity', fontweight = 'bold', fontsize = 16)
plt.show()

"""As you can see in above graph

It shows as the Salary Increases the Years of Experience increases variation is ocnstant along the way in data

The data doesn't have any specific pattern in the variation. hence, we can say it's Homoscedasticity
"""

df.var()

"""Step 6:
Feature Engineering

Trying different transformation of data to estimate normal distribution and remove any skewness
"""

sns.distplot(df['YearsExperience'], bins = 10, kde = True)
plt.title('Before Transformation')
sns.displot(np.log(df['YearsExperience']), bins = 10, kde = True)
plt.title('After Transformation')
plt.show()

labels = ['Before Transformation','After Transformation']
sns.distplot(df['YearsExperience'], bins = 10, kde = True)
sns.distplot(np.log(df['YearsExperience']), bins = 10, kde = True)
plt.legend(labels)
plt.show()

"""As you can see

How log transformation affects the data and it scales the values down.

Before prediction it is necessary to reverse scaled the values, even for calculating RMSE for the models.(Errors)
"""

smf.qqplot(np.log(df['YearsExperience']), line = 'r')
plt.title('No transformation')
smf.qqplot(np.sqrt(df['YearsExperience']), line = 'r')
plt.title('Log transformation')
smf.qqplot(np.sqrt(df['YearsExperience']), line = 'r')
plt.title('Square root transformation')
smf.qqplot(np.cbrt(df['YearsExperience']), line = 'r')
plt.title('Cube root transformation')
plt.show()

labels = ['Before Transformation','After Transformation']
sns.distplot(df['Salary'], bins = 10, kde = True)
sns.displot(np.log(df['Salary']), bins = 10, kde = True)
plt.title('After Transformation')
plt.show()

smf.qqplot(df['Salary'], line = 'r')
plt.title('No transformation')
smf.qqplot(np.log(df['Salary']), line = 'r')
plt.title('Log transformation')
smf.qqplot(np.sqrt(df['Salary']), line = 'r')
plt.title('Square root transformation')
smf.qqplot(np.cbrt(df['Salary']), line = 'r')
plt.title('Cube root transformation')
plt.show()

"""Step 7:
Fitting a Linear Regression Model
Using Ordinary least squares (OLS) regression
"""

import statsmodels.formula.api as sm
model = sm.ols('Salary~YearsExperience', data = df).fit()

model.summary()

"""As you can notice in the above model

The R-squared and Adjusted R-squared scores are above 0.85.

(It is a thumb rule to consider Adjusted R-squared to be greater than 0.8 for a good model for prediction)

F-statitics is quite high as well and yes desire it to be higher
But log-likelihood is quite very low far away from 0
and AIC and BIC score are much higher for this model

Lets Try some data transformation to check whether these scores can get any better than this.

Square Root transformation on data
"""

model1 = sm.ols('np.sqrt(Salary)~np.sqrt(YearsExperience)', data = df).fit()
model1.summary()

"""As you can notice in the above model

The R-squared and Adjusted R-squared scores are above 0.85. but its has gotten less than previous model

(It is a thumb rule to consider Adjusted R-squared to be greater than 0.8 for a good model for prediction)

F-statitics has gotten a little lower for this model than previous.

But log-likelihood got better than before close to 0 higher than previous model
and AIC and BIC score are now much better for this model

Lets Try some data transformation to check whether these scores can get any better than this.

Cuberoot transformation on Data
"""

model2 = sm.ols('np.cbrt(Salary)~np.cbrt(YearsExperience)', data = df).fit()
model2.summary()

"""Log transformation on Data"""

model3 = sm.ols('np.log(Salary)~np.log(YearsExperience)', data = df).fit()
model3.summary()

"""Model Testing

As Y = Beta0 + Beta1*(X)

Finding Coefficient Parameters (Beta0 and Beta1 values)
"""

model.params

"""Here, (Intercept) Beta0 value = 25792.20 & (YearsExperience) Beta1 value = 9449.96

Hypothesis testing of X variable by finding test_statistics and P_values for Beta1 i.e if (P_value < α=0.05 ; Reject Null)

Null Hypothesis as Beta1=0 (No Slope) and Alternate Hypthesis as Beta1≠0 (Some or significant Slope)
"""

print(model.tvalues,'\n',model.pvalues)

"""(Intercept) Beta0: tvalue=11.34 , pvalue=5.511950e-12

(daily) Beta1: tvalue=24.95, pvalue= 1.143068e-20

As (pvalue=0)<(α=0.05); Reject Null hyp. Thus, X(YearsExperience) variable has good slope and variance w.r.t Y(Salary) variable.

R-squared measures the strength of the relationship between your model and the dependent variable on a 0 – 100% scale.

Measure goodness-of-fit by finding rsquared values (percentage of variance)
"""

model.rsquared,model.rsquared_adj

"""Determination Coefficient = rsquared value = 0.95 ; very good fit >= 85%

Step 8:
Residual Analysis

Test for Normality of Residuals (Q-Q Plot)
"""

import statsmodels.api as sm
sm.qqplot(model.resid, line = 'q')
plt.title('Normal Q-Q plot of residuals of Model without any data transformation')
plt.show()

"""As you can notice in the above plot

The first model follows normal distribution

Residual Plot to check Homoscedasticity or Hetroscedasticity
"""

def get_standardized_values( vals ):
    return (vals - vals.mean())/vals.std()

plt.scatter(get_standardized_values(model.fittedvalues), get_standardized_values(model.resid))
plt.title('Residual Plot for Model without any data transformation')
plt.xlabel('Standardized Fitted Values')
plt.ylabel('Standardized Residual Values')
plt.show()

"""As you can notice in the above plots

The Model have Homoscedasciticity.

The Residual(i.e Residual = Actual Value - Predicted Value) and the Fitted values do not share any Pattern.

Hence, there is no relation between the Residual and the Fitted Value. It is Randomly distributed

Step 9:
Model Validation

We will analyze Mean Squared Error (MSE) or Root Mean Squared Error (RMSE) — AKA the average distance (squared to get rid of negative numbers) between the model’s predicted target value and the actual target value.

Comparing different models with respect to the Root Mean Squared Errors
"""

from sklearn.metrics import mean_squared_error

model1_pred_y =np.square(model1.predict(df['YearsExperience']))
model2_pred_y =pow(model2.predict(df['YearsExperience']),3)
model3_pred_y =np.exp(model3.predict(df['YearsExperience']))

model1_rmse =np.sqrt(mean_squared_error(df['Salary'], model1_pred_y))
model2_rmse =np.sqrt(mean_squared_error(df['Salary'], model2_pred_y))
model3_rmse =np.sqrt(mean_squared_error(df['Salary'], model3_pred_y))
print('model=', np.sqrt(model.mse_resid),'\n' 'model1=', model1_rmse,'\n' 'model2=', model2_rmse,'\n' 'model3=', model3_rmse)

rmse = {'model': np.sqrt(model.mse_resid), 'model1': model1_rmse, 'model2': model3_rmse, 'model3' : model3_rmse}
min(rmse, key=rmse.get)

"""As model has the minimum RMSE and highest Adjusted R-squared score. Hence, we are going to use model to predict our values

Model is that Simple Linear regression model where we did not perfrom any data transformation and got the highest Adjusted R-squared value

Step 10:
Predicting values
"""

# first model results without any transformation
predicted2 = pd.DataFrame()
predicted2['YearsExperience'] = df.YearsExperience
predicted2['Salary'] = df.Salary

predicted2



